{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f5427f",
   "metadata": {},
   "source": [
    "# SLP\n",
    "## Binary Classification\n",
    "\n",
    "### Binary Classification의 신경망 처리\n",
    "이진 판단 문제는 예/아니오 혹은 0/1 같은 두가지 값 중 하나로 답하는 문제이다. 이진 판단 문제를 인공지능 신경망으로 처리하는 일은 생각보다 까다롭다. 우선 가중치와 편향을 이용하는 퍼셉트로느이 선형 연산은 기본적으로 두 가지 값으로 결과를 제한할 수 없다. 이에 따라 초기에는 선형 연산 결과가 임계치를 넘는지에 따라 두 가지 값 중 하나로 출력하는 방법이 고려되었으나 미분이 불가능하여 학습이 어려워지는 문제가 있었다.\n",
    "\n",
    "이 때문에 신경망은 확률에 해당하는 값을 추정하고 이 값이 1에 가까우면 참, 0에 가까우면 거짓을 선택하는 방식을 생각하게 되었다. 하지만 퍼셉트론의 선형 연산 자체는 0과 1 사이의 확률값으로 출력 범위를 제한하는 것마저도 불가능하다. 이에따라 선형 연산에서는 일단 범위에 제한이 없는 실숫값을 생산하고 이를 확률값의 성질에 맞게 변환해주는 비선형 함수를 찾아 이용하게 되었다. 이것이 sigmoid function이다.\n",
    "\n",
    "sigmoid 함수를 이용해 신경망 출력을 확률로 해석하고, 손실함수를 정의하기 위해 많은 시행착오를 겪었다. 앞선 MSE에서 설명한 것 처럼 딥러닝에서는 값이 0 이상이면서 추정이 정확해질수록 작아지는 성질이 있는 손실함수를 정의할 수 있어야 한다. 이 문제를 해결해 준 것은 두가지 확률 분포가 얼마나 다른지를 숫자 하나로 표현해주는 교차 엔트로피라는 개념이었다. 항상 양수이고 두 확률 분포가 비슷해질수록 값이 작아지는 교차 엔트로피 값은 binary classification에 대한 손실 함수로 적합했고, 그 덕분에 시그모이드 함수의 교차 엔트로피 값을 손실 함수로 정의하는 방법을 통해 신경망 학습이 가능하게 되었다.\n",
    "\n",
    "### Sigmoid function\n",
    "Sigmoid function은 범위에 제한 없는 임의의 실숫값을 입력으로 받아 확률값의 범위에 해당하는 0과 1 사이의 값을 출력하는 함수이다. 시그모이드 함수는 입력 x가 어떤 확률값의 logit 표현이라고 간주한다. 여기서 logit이란 실제 표현하려는 값을 로그값으로 대신 나타낸 것이다. sigmoid function은 이 logit값 x를 확률값으로 변환해주는 함수이다.\n",
    "\n",
    "sigmoid function은 입력값을 '답이 참일 가능성을 logit으로 표시한 값'으로 간주한다. 그런데 logit은 상대적인 값이므로 정확한 의미를 알려면 비교대상이 필요하다. sigmoid function은 남은 또 하나의 가능성인 답이 거짓일 경우에 대한 logit값을 0으로 간주하는 방법으로 sigmoid function 입력값이 나타내는 확률의 정확한 값을 구할 근거를 마련한다.\n",
    "\n",
    "ex) logit으로 표시된 가능성과 true일 확률\n",
    "    - 주어진 조건이 true일 가능성은 logit 값으로 0.5이다.\n",
    "        true 가능성 : false 가능성 = e^0.5 : e^0 = 1.649 : 1\n",
    "        ture 확률 = 1.649 / (1.649 + 1) = 0.622\n",
    "\n",
    "위의 예시를 일반화하면 시그모이드 함수가 어떤 모양으로 정의되어야 하는지를 알 수 있다. 답이 true일 가능성과 flase일 가능성의 logit값이 각각 x와 0이므로 거짓인 경우의 실제확률을 계산하여 확률을 구해보면 우리가 알고 있는 시그모이듸 함수의 정의식이 도출된다.\n",
    "\n",
    "### 확률 분포와 정보 엔트로피\n",
    "엔트로피는 원래 분자들의 무질서도 혹은 에너지의 분산 정도를 나타내는 물리학 용어이다. 이를 활용하여 확률 분포의 무질서도나 불확실성(uncertainty) 혹은 정보 표현의 부담 정도를 나타내는 정보 엔트로피 개념이 등장한다.\n",
    "\n",
    "m가지 결과가 각각 $p_1, p_2, p_3, .... p_m$의 확률로 일어나는 무작위적인 사건의 결과를 효율적으로 표현해야 하는 경우를 가정하자. 만약 $p_1 = p_2 = p_3 = \\frac{1}{3}$ 이라면 1비트 만으로는 세가지 결과를 나타낼 수 없어 2비트를 사용하게 된다. 하지만 이런 경우 사건이 독립적으로 여러차례 발생하여 다수의 결과를 함께 표현하면서 정보를 압축할 수 있다면 이러한 낭비를 줄일 수 있다. 이 사건이 k 차례 일어난다면 경우의 수는 $3^k$이 되며 n 비트로 이들 겨로가를 표현하려 한다면 최소한 $2^n >= 3^k$여야 한다. 따라서 $\\frac{n}{k} >= log_{2}{3}$ 이므로 사건 하나를 표현하는데 필요한 최소한의 비트수는 $log_{2}[3] \\approx 1.585$이다. 마찬가지로 $p_1 = ... = p_m = \\frac{1}{m}$일 때 한 사건을 표현하는 데 필요한 최소 비트 수는 $log_{2}{m}$이며 이 값을 정보량이라고 한다.\n",
    "\n",
    "이러한 정보량 개념은 각 격우에 대한 확률이 균일하지 않은 확률 분포에도 혹장하여 적용할 수 있다. 일반적으로 어떤 사건이 $p_1 = \\frac{1}{a_1}, p_2 = \\frac{1}{a_2}, ... , p_m  = \\frac{1}{a_m}$의 확률 분포를 가질 때 이 사건의 정보량은 각 경우가 갖는 정보량을 각 경우의 확률에 다라 가중 평균하여 아래와 같이 구할 수 있다.\n",
    "\n",
    "$$\\frac{1}{a_1}log_{2}{a_1} + \\frac{1}{a_2}log_{2}{a_2} + ... + \\frac{1}{a_m}log_{2}{a_m}$$\n",
    "\n",
    "정보량을 정의하는 $\\displaystyle\\sum_{i=1}^{m}{\\frac{1}{a_i}log_{2}{a_i}}$의 식을 주어진 확률 분포에 대한 정보 엔트로피라고 하고 보통 H로 표시하는데 $p_i = \\frac{1}{a_i}$의 역수 관계를 이용하여 아래와 같이 바꾸어 표현할 수 있다.\n",
    "\n",
    "$$H = \\displaystyle\\sum_{i=1}^{m}{p_ilog_{2}{\\frac{1}{p_i}}} = \\displaystyle\\sum_{i=1}^{m}{p_i(-log_{2}{p_i})} = -\\displaystyle\\sum_{i=1}^{m}{p_i(log_{2}{p_i})}$$\n",
    "\n",
    "이렇게 정의되는 정보 엔트로피는 어떤 확률 분포로 일어나는 사건을 표현하는 데 필요한 정보량이며 이 값이 커질수록 확률 분포의 불확실성이 커지며 결과에 대한 예측이 어려워진다.\n",
    "\n",
    "### 확률 분포의 추정과 교차 엔트로피\n",
    "\n",
    "정보 엔트로피가 하나의 확률 분포가 갖는 불확실성 혹은 정보량을 정량적으로 계산할 수 있게 하는 개념이라면 교차 엔트로피는 두 확률 분포가 얼마나 비슷한지를 숫자 하나로 나타내는 개념이다.\n",
    "\n",
    "앞선 $H = \\sum{p_ilog{p_i}}$로 정의된 정보 엔트로피는 정보량의 기댓값이라고도 말할 수 있다. 확률 p를 갖는 항의 정보량은 $-log{p_i}$로 표현할 수 있다. $H = \\sum{p_ilog{p_i}}$는 각 경우의 발생 확률에 따라 정보량의 가중 평균을 구한다는 의미로 해석할 수 있다. 확률분포를 갖는 어떤 값의 기댓값이란 각 경우의 값을 확률에 따라 가중평균한 값으로 정의된다. 따라서 H는 정보량의 기댓값에 해당한다.\n",
    "\n",
    "교차 엔트로피는 정보 엔트로피의 이런 해석을 약간 수정하여 정보량을 제공하는 확률 분포와 가중평균 계산에 사용되는 확률 분포를 서로 다르게 설정한 채 정보량의 기댓값을 구하는 방식으로 정의된다. 즉 $q_1, q_2, ... , q_n$의 확률 분포 Q에 따른 정보량을 갖는 사건이 $p_1, p_2, ... p_n$의 확률 분포 P에 따라 일어날 때 정보량의 가중평균 $H(P, Q) = -\\sum{p_ilog{q_i}}$를 P에 대한 Q의 교차 엔트로피로 정의한다.\n",
    "\n",
    "$H(P, P) = -\\sum{p_ilog{p_i}} = H$이므로 교차 엔트로피는 두 확률 분포가 같은 내용을 가질때(P와 Q의 확률 분포가 같을 경우) 해당 확률 분포의 정보 엔트로피값과 같아진다. 또한 언제나 $H(P, Q) \\geq H(P, P)$가 성립하며 특히 등호는 두 확률 분포가 일치할 때만 성립한다. 교차 엔트로피는 두 확률 분포가 닮아갈수록 값이 작아지기에 두 확률 분포가 얼마나 다른지를 나타내는 정량적 지표 역할을 한다.\n",
    "\n",
    "### 딥러닝 학습에서의 교차 엔트로피\n",
    "\n",
    "학습 중 수시로 수정되는 모델의 추정 확률 분포 P가 있고, 이 딥러닝 모델이 흉내 내야 할 미지의 확률분포 Q가 있다. Q와 P의 교차 엔트로피 값을 계산할 수 있거나 최소한 추정이라도 할 수 있다면 이 교차 엔트로피값이 작아지는 방향으로 P를 꾸준히 수정함으로써 P를 Q에 근접시켜 갈 수 있다. 이 IDEA가 Binary Classificatoin에서 신경망을 학습시킬 수 있는 원리가 되지만, 실제 학습에는 논리적으로 두가지 어려움이 있다.\n",
    "\n",
    "#### 논리적 어려움 2가지\n",
    "1. 애당초 Q를 정확히 알지 못하는 상태이기에 Q와 P의 교차 엔트로피를 계산할 수 없다. 확률 분포 Q는 딥러닝 모델이 학습을 통해 찾아내고 닮아가야 할 목표이지 학습을 시작할 때 갖고 출발하는 준비물이 아니다. 또한 data set은 실세계의 모든 data 중 일부를 모아둔 샘플일 뿐이다. 따라서 data set data로부터 확률분포 Q를 찾아낸다 해도 이는 전체 데이터의 확률 분포를 대변하지 않는다.\n",
    "\n",
    "2. 학습에 이용되는 DATA들은 각각 다른 입력과 그에 따른 출력이라서 확률 분포 Q는 고정된 확률 분포가 아닌 입력에 따라 달라지는 조건부 확률 분포이다. 딥러닝 신경망으로 실제 문제를 풀 때 문제 내용에 따라 답을 예측해야지 고정된 확률 분포로 답을 도출해서는 안된다는 점이다.\n",
    "\n",
    "모범 답안으로 주어지는 LABEL 정보를 입력에 따른 조건부 확률 분포로 P를 재해석하고 이를 이용하여 교차 엔트로피를 계싼하면 이 두가지 어려움을 해결할 수 있다. 이처럼 LABLE 정보의 내용을 추정 확률 분포 P가 접근해가야 할 학습 목표, 즉 입력에 대한 출력의 조건부 확률 분포로 간주하면 교차 엔트로피 계산을 할 수 있다.\n",
    "\n",
    "딥러닝 학습의 궁극적인 목표는 아직 접해보지 못한 새로운 입력에 대해서도 유사한 문제 풀이 경험을 토대로 알맞은 답을 내놓는 것이다. 그런데 이진 판단 딥러닝에서는 입력에 대해 직접 출력을 추정하지 않는다. 대신 출력의 확률 분포, 즉 결과가 참 혹은 거짓일 가능성을 입력에 따른 조건부 확률 분포로서 추정한다. 입력 데이터와 입력에 대한 출력값의 조건부 확률 분포 사이에는 어떤 상관관계 패턴이 숨어 있을 것이다. 다양한 입출력 쌍에 대해 교차 엔트로피를 줄이는 학습을 반복하다 보면 이 숨어 있는 패턴이 신경망에 반영되어 각각의 입력에 알맞은 조건부 확률 분포를 적절하게 만들어낼 수 있을 것이다.\n",
    "\n",
    "이처럼 Binary Classification 문제에서는 답 자체보다 답이 나오게 되는 확률 분포가 문제의 핵심이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
