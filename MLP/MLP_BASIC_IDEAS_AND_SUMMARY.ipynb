{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710eef32",
   "metadata": {},
   "source": [
    "# MLP(다층 퍼셉트론 신경망)\n",
    "\n",
    "다층 퍼셉트론 신경망은 복수의 퍼셉트론 계층을 순서를 두고 배치하여 입력 벡터로부터 중간 표현을 거쳐 출력 벡터를 얻어내는 신경망 구조이다. 간단히 다층 퍼셉트론이라고 부른다.\n",
    "\n",
    "<img src = \"MLP_IMAGE1.png\" width = \"700px\">\n",
    "\n",
    "위의 그림은 Hidden layer가 2개인 Multi layer perceptron의 구성 예시이다. 다층 퍼셉트론에서 새로 추가되는 계층들은 출력 계층 앞단에 배치되어 입력 벡터를 차례로 처리한 뒤 다음 단계로 넘겨 출력 계층까지 이르게 한다.\n",
    "\n",
    "다층 퍼셉트론에서 각각의 계층(layer)는 perceptron과 같은 내부 구조를 갖는다. 즉, 하나의 계층(layer) 안에 속한 perceptron들은 동일한 입력을 공유하면서 각각 출력 성분을 만들어내지만 서로 어떠한 연결도 없어 영향을 주고 받을 수 없다. 하지만 이와 반대로 인접한 layer(계층)끼리는 방향성을 갖는 완전 연결 방식으로 연결된다.\n",
    "\n",
    "엄밀하게는 SLP(Single Layer Perceptron)도 0개의 은닉 계층(hidden layer)를 가지는 Multi Layer Percetron의 special case로 생각할 수도 있따. MLP 신경망은 SLP에 비해 더 많은 Perceptron을 이용하여 기억 용량이나 계산 능력에 대한 부담이 커지는 대신 품질 향상을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8e53a",
   "metadata": {},
   "source": [
    "## Hidden Lyae의 수와 폭\n",
    "\n",
    "MLP에서 최종 단계에 배치된 Layer는 신경망에 주어진 원래 임무(task)에 따라 알맞은 형태의 출력 벡터를 생성하는 역할을 맡는다. 그래서 출력 계층이라는 별도의 이름을 지니며 출력 벡터의 크기, 즉 출력 계층이 가질 Perceptron의 수도 문제의 성격에 따라 고정적으로 정해진다.\n",
    "\n",
    "반면 MLP에서 새로 도입된 hidden layer가 만들어낼 hidden vector에는 이러한 제약이 없다. 따라서 출력 계층과 달리 hidden layer의 수와 각 hidden layer의 폭은 신경망 설계자가 자유롭게 정할 수 있다. 여기서 hidden layer의 폭이란 각 계층이 갖는 perecptron의 수이자 생성하는 hidden vector의 크기를 의미한다. 또한 perceptron을 node라 한다.\n",
    "\n",
    "hidden layer의 수와 hidden layer의 폭은 신경망의 품질을 결정하는 중요한 요인이 될수 있지만, 무조건 hidden layer의 수나 폭을 늘린다고 품질이 좋아지는 것은 아니다. hidden layer를 추가하여 parameter의 수가 늘어나면 더 많은 학습 데이터가 필요해지는 경향이 있으며, 따라서 충분한 양의 data가 준비되지 않으면 MLP 구조의 확장은 오히려 신경망의 품질을 하락시킨다.\n",
    "\n",
    "Hidden layer의 수와 폭은 문제의 규모, 데이터양, 난이도를 종합적으로 고려해 정해야 한다. 그래서 다양한 실험과 축적된 경험이 중요하다. 이때 학습률이나 미니배치 크기와 같은 다른 하이퍼 파라미터도 중요한 영향을 미칠 수 있끼에 유의해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aacae1",
   "metadata": {},
   "source": [
    "## 비선형 활성화 함수\n",
    "\n",
    "은닉 계층은 가중치와 편향을 이용해 계산된 선형 연산 결과를 바로 출력으로 내보내는 대신 한 번 더 변형시켜 내보낸다. 선형 연산 결과 뒷단에 적용되어 퍼셉트론의 출력을 변형시키려고 추가한 장치를 비선형 활성화 함수라고 한다. 선형 연산 결과는 곱셈, 덧셈으로만 이루어지는 선형 연산의 특성상 항상 입력의 일차 함수로 나타나기 마련이다. 비선형 함수는 일차함수로는 표현이 불가능한 좀 더 복잡한 기능을 수행하기위해 사용하는 함수이다. 따라서 비선형 활성화 함수를 추가하면 입력의 일차 함수 표현을 넘어서는 다양하고 복잡한 형태의 퍼셉트론 출력을 만들 수 있따. (Sigmoid function, softmax function ... 이 이러한 비선형 활성화 함수에 해당한다.)\n",
    "\n",
    "MLP에서 비선형 활성화 함수는 필수적 구성 요소이다. 애써서 MLP를 도입해도 비선형 활성화 함수가 없다면 아무런 의미가 없다. 이는 선형 처리는 아무리 여러 단계를 반복해도 하나의 선형 처리로 줄여 표현할 수 있다는 수학적 원리 때문이다. 비선형 활성화 함수가 없다면 언제나 다층 퍼셉트론과 똑같은 기능의 slp를 만들어 낼 수 있기에 굳이 MLP를 만들 필요가 없다는 의미이다.\n",
    "\n",
    "하지만 적당한 비선형 함수 $\\varphi$를 도입하여 $h_i = \\varphi(w_{i_1}x_{1} + ... + w_{i_n}x_{n} + b)$ 로 바꾸면 이러한 선형성의 한계에서 벗어날 수 있다. 이것만이 아니다. 비선형 활성화 함수를 갖춘 hidden layer를 충분한 수의 perceptron으로 구성하고 가중치와 편향값만 잘 설정해주면 단 두 계층의 다층 퍼셉트론 구조만으로 어떤 수학적 함수이든 원하는 오차 수준 이내로 근사하게 동작할 수 있음이 수학적으로 증명되었다.\n",
    "\n",
    "물론 이런 수학적 증명에도 불구하고 은닉 계층이 하나일 때 문제 내용이 복잡하면 은닉 계층에 필요한 perceptron 수가 기하 급수적으로 늘어날 수 있다. 하지만 여러 실험에 따르면 노드 수가 많은 SLP 신경망보다 노드 수가 적은 MLP 신경망 성능이 훨씬 우수한 경우가 많다. 이는 입력 데이터로부터 유용한 정보를 추출해내는 추상화에 단층 구조보다 다층 구조가 효과적이기 때문이다. 이런 이유에 의거하여 Layer 수는 적지만 전체적으로 오히려 많은 node를 갖는 신경망 보다 layer 수는 많아도 node가 적은 신경망으로 문제를 해결하려는 경향이 나타나면서 deep learning의 deep이 부각되었다.\n",
    "\n",
    "Deep learning의 연구가 활발해지면서 우수한 품질을 보이는 parameter 값의 조합을 찾는 방법에 대한 연구가 부단히 이어졌다. Gradient Descent나 Adam Algroithm은 이러한 연구 결과 찾아낸 해결책이다. 하지만 deep learning이 비록 여러 분야에서 우수한 성능을 보이고 있따 해도 왜 이런 우수한 성능이 얻어질 수 있는지가 수학적으로 증명된 것은 아니다.\n",
    "\n",
    "요약 하자면 MLP 구조와 비선형 활성화 함수의 도입은 단층 Perceptron만으로 해결할 수 없던 여러 문제들을 신경망이 풀어낼 수 있도록 해주었다. MLP 구조의 남은 문제는 부족한 data로 자칫 지나치게 커지기 쉬운 신경망을 어떻게 학습시켜 적절한 parameter 조합을 찾느냐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80b753",
   "metadata": {},
   "source": [
    "## ReLU Function\n",
    "\n",
    "ReLU(Rectified Linear Unit) 함수는 음수 입력을 걸러내 0으로 만드는 간단한 기능을 제공한다. Hidden layer의 비선형 활성화 함수로 가장 널리 이용되는 함수이다.\n",
    "\n",
    "Sigmoid, Softmax도 비선형 함수이지만, 지수 연산이 포함된 복잡한 계산과정으로 처리 부담이 크다. 게다가 Softmax 함수는 벡터 원소들을 한데 묶어 처리하기에 은닉 계층 출력 처리에는 적절하지 않다. 이진 판단이나 선택 분류의 후처리에서는 확률 분포와 관련하여 수학적 의미가 가장 타당한 이러한 함수를 사용해야 하지만, MLP에서 Hidden layer에 비선형적 특성을 추가하는데에 굳이 이런 함수를 이용할 필요는 없기에 쉽고 빠르게 연산할 수 있는 ReLU 함수를 주로 사용한다.\n",
    "\n",
    "\n",
    "(ReLU의 문제점)\n",
    "\n",
    "ReLU 함수의 문제는 x = 0에서 미분이 불가능하다는 것이다. 하지만 이렇게 한 점에서 미분 불능인 경우는 그 지점에서의 미분값을 강제로 정해주고 넘어가는 경우가 많다. ReLU에서 x = 0에서의 기울기를 대체로 0으로 설정해주고 ReLU 함수를 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ecca1",
   "metadata": {},
   "source": [
    "## 민스키의 XOR 문제와 비선형 활성화 함수\n",
    "\n",
    "민스키는 저서 Perceptrons에서 Perceptron을 단층으로 연결해서는 기본적인 논리 연산인 XOR 함수 기능을 구현할 수 없음을 지적하였다.\n",
    "XOR 연산은 입력 x1, x2에 대해서 XOR 연산 결과로 작동하는 함수를 의미한다. 즉 이진 입력 中 하나만 값이 1이면 1을 출력하고 그렇기 않으면 0을 출력하는 함수를 의미한다.\n",
    "\n",
    "<img src = \"SLP_XOR_PROVE.png\" width = \"700px\">\n",
    "\n",
    "증명과정을 요약하자면, $x_2$값에 따라서 weight가 달라지도록 구현할 수 있어야 XOR Function 기능을 수행할 수 있는데, 자기가 볼 수도 없는 $x_2$의 값에 따라 증감 방향을 유연하게 바꾸어 줄 수 없기 때문에 불가능하다.\n",
    "\n",
    "이러한 민스키의 지적은 한 때 신경망 연구 분야에 위축을 가져왔으나 사실 이는 단층 Perceptron의 선형 처리에서 기인하는 문제이다. 신경망 연구자들은 이 후 비선형 활성화 함수를 동반한 MLP라는 탈출구를 찾아냈다.\n",
    "\n",
    "<img src = \"MLP_XOR_WITH_RELU.png\" widht = \"500px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf6566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d7232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
